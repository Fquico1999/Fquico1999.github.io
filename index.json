[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m Francisco, a fourth year Engineering Physics student at UBC who\u0026rsquo;s passionate about tackling real world problems with the exciting potential of Machine Learning. While you\u0026rsquo;re here, check out some of my projects, view my resume, or drop me a line below.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://Fquico1999.github.io/author/francisco-farinha/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/francisco-farinha/","section":"authors","summary":"I\u0026rsquo;m Francisco, a fourth year Engineering Physics student at UBC who\u0026rsquo;s passionate about tackling real world problems with the exciting potential of Machine Learning. While you\u0026rsquo;re here, check out some of my projects, view my resume, or drop me a line below.","tags":null,"title":"Francisco Farinha","type":"authors"},{"authors":[],"categories":[],"content":"You can find this repository here\nOverview In an attempt to test and further my understanding of the mathematics and logistics behind neural networks and how they operate, I decided to follow what I learned in deeplearning.ai\u0026rsquo;s Neural Networks and Deep Learning course and implement Neural Networks from scratch using only NumPy.\nOutline To build a neural net from scratch, we need to go over each block and code those individually. At the end we can combine all of these to create an $L$-layer NN.\nSo, the steps we need to take are:\n Parameter Intialization: We need to initialize parameters $W$ and $b$ Compute a forward propagation pass: This involves computing the linear pass - $Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$ - and the activation $A^{[l]}=g(Z^{[l]})$ for both Sigmoid and ReLU activations Compute the loss Implement a back propagation pass Update the parameters: Here I'll code in mini Batch Gradient Descent (Which will cover both Stochastic Gradient Descent as well as Batch Gradient Descent), Momentum, RMSProp, and the king of them all, Adam  import math import numpy as np import matplotlib.pyplot as plt  Activation Functions To add non-linearity to the model, activation functions are used. I\u0026rsquo;ll define them now. I\u0026rsquo;ll be using ReLU (rectified linear unit) and sigmoid in an example, but I\u0026rsquo;ll also define tanh and leaky ReLU.\ndef relu(Z): \u0026quot;\u0026quot;\u0026quot; Arguments: Z -- output of linear function Z = W*A+b Returns: ret -- ReLU(Z) Z -- input for use in backprop \u0026quot;\u0026quot;\u0026quot; return np.maximum(0,Z), Z def sigmoid(Z): \u0026quot;\u0026quot;\u0026quot; Arguments: Z -- output of linear function Z = W*A+b Returns: ret -- sigmoid(Z) Z -- input for use in backprop \u0026quot;\u0026quot;\u0026quot; return 1./(1.+np.exp(-Z)), Z def tanh(Z): \u0026quot;\u0026quot;\u0026quot; Arguments: Z -- output of linear function Z = W*A+b Returns: ret -- tanh(Z) Z -- input for use in backprop \u0026quot;\u0026quot;\u0026quot; return np.tanh(Z), Z def leaky_relu(Z): \u0026quot;\u0026quot;\u0026quot; Arguments: Z -- output of linear function Z = W*A+b Returns: ret -- leaky_relu(Z) Z -- input for use in backprop \u0026quot;\u0026quot;\u0026quot; return np.maximum(0.01*Z, Z), Z  Parameter Initialization For passing parameter information between different functions, I\u0026rsquo;ll use a dictionary parameters, which will store $W$ and $b$ values for each layer $l {l:{0\\le l \\le L}}$\nAdditionally, I\u0026rsquo;ll implement random, Xavier initialization, and He initialization.\n Random Initialization: Samples values from a normal distribution, and multiplies by a small value to keep weights close to zero - regularization Xavier Initialization: random sampling is multiplied by constant $\\sqrt{\\frac{1}{\\text{previous layer dimension}}}$ He Initialization: random sampling is multiplied by constant $\\sqrt{\\frac{2}{\\text{previous layer dimension}}}$  def initialize_parameters(model_shape, initialization_method='he'): \u0026quot;\u0026quot;\u0026quot; Initializes parameters W and b of a network of shape model_shape. Arguments: model_shape -- list containing the dimensions of each network layer l Returns: parameters -- dictionary containing weight and bias parameters \u0026quot;\u0026quot;\u0026quot; #define dictionary params = {} #Obtain L L = len(model_shape) #Check initialization_method if initialization_method == 'random': beta = 0.01 for l in range(1,L): params[\u0026quot;W\u0026quot;+str(l)] = np.random.randn(model_shape[l], model_shape[l-1])*beta params[\u0026quot;b\u0026quot;+str(l)] = np.zeros([model_shape[l], 1]) elif initialization_method == 'xavier': L = L-1 for l in range(1,L+1): beta = np.sqrt(1./model_shape[l-1]) params[\u0026quot;W\u0026quot;+str(l)] = np.random.randn(model_shape[l], model_shape[l-1])*beta params[\u0026quot;b\u0026quot;+str(l)] = np.zeros([model_shape[l], 1]) elif initialization_method == 'he': L = L - 1 for l in range(1,L+1): beta = np.sqrt(2./model_shape[l-1]) params[\u0026quot;W\u0026quot;+str(l)] = np.random.randn(model_shape[l], model_shape[l-1])*beta params[\u0026quot;b\u0026quot;+str(l)] = np.zeros([model_shape[l], 1]) else: raise NameError(\u0026quot;%s is not a valid initalization method\u0026quot;%(initialization_method)) return params  Forward Propagation Forward propagation refers to passing through the computation graph from left to right - forwards - and evaluating $Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$ for each sucessive $l$ starting with $l=1$, in which case $A^{[0]}=X$, in other words, the activation fed into the first layer is simply the inputs.\nTo accomplish this, I\u0026rsquo;ll create two functions. The first will evaluate the linear formula $Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$, whereas the second will evaluate $A^{[l]} = g(Z^{[l]})$, which corresponds to evaluating the activation function.\nThen forward_prop implements both to complete a forward propagation pass.\nIn order to compute the backprop later onwards, I\u0026rsquo;ll need to store $A^{[l]}$,$W^{[l]}$, $b^{[l]}$ as well as $Z^{[l]}$ which I\u0026rsquo;ll do in linear cache and activation cache\nOne of the arguments of forward_prop is layer_activations, which is a list of the activations for each layer of the neural network.\ndef forward_linear(W,A,b): \u0026quot;\u0026quot;\u0026quot; Linear part of forward propagation Arguments: W -- weight matrix A -- activations b -- bias matrix Returns: Z -- input to the layer's activation function linear_cache -- tuple with A, W, b for efficient backprop \u0026quot;\u0026quot;\u0026quot; Z = np.dot(W,A)+b linear_cache = (A,W,b) assert(Z.shape == (W.shape[0], A.shape[1])) return Z, linear_cache  def forward_activation(Z, activation): \u0026quot;\u0026quot;\u0026quot; Arguments: Z -- Output of linear function Z = WA_prev+b activation -- String denoting activation function to use. One of [linear, sigmoid, relu, leaky_relu, tanh, softmax] Returns: A -- g(Z), where g() is the corresponding activation activation_cache -- the input Z, which will be fed into backprop \u0026quot;\u0026quot;\u0026quot; if activation == 'linear': A, activation_cache = Z, Z elif activation == 'sigmoid': A, activation_cache = sigmoid(Z) elif activation == 'relu': A, activation_cache = relu(Z) elif activation == 'leaky_relu': A, activation_cache = leaky_relu(Z) elif activation == 'tanh': A, activation_cache = tanh(Z) else: raise NameError('%s is not a valid activation function' %(activation)) return A, activation_cache  def forward_prop(X, layer_activations, parameters): \u0026quot;\u0026quot;\u0026quot; Implements one pass of forward propagation Arguments: X -- input data layer_activations -- list of strings corresponding to the activations of each layer parameters -- output of initialize_parameters Returns: A - Output of activation function of the last layer caches - list of caches containing both linear and activation caches \u0026quot;\u0026quot;\u0026quot; #Define caches caches = [] #A[0] is the input A = X L = len(parameters)//2 for l in range(1, L+1): A_prev = A W = parameters[\u0026quot;W\u0026quot;+str(l)] b = parameters[\u0026quot;b\u0026quot;+str(l)] Z, linear_cache = forward_linear(W, A_prev, b) A, activation_cache = forward_activation(Z, layer_activations[l]) assert (A.shape == (W.shape[0], A_prev.shape[1])) #Add both linear and activation cache to caches caches.append((linear_cache, activation_cache)) return A, caches  Cost Function The cost function is the metric that a neural net aims to minimize. I\u0026rsquo;ll implement cross-entropy cost, given by:\n$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$\nThus, we require a method of computing cost after one pass of forward propagation.\ndef cost(A_last, Y): \u0026quot;\u0026quot;\u0026quot; Arguments: A_last -- Post-activation value of the last layer of the network Y -- Groud truth vectors Returns: cost -- cross-entropy cost \u0026quot;\u0026quot;\u0026quot; #Get number of samples, m m = Y.shape[1] #Compute cross entropy cost cost = -(1.0/m)*np.sum(Y*np.log(A_last)+(1.-Y)*np.log(1.-A_last)) #Ensure appropriate dimensions cost = np.squeeze(cost) return cost  Back Propagation To update our parameters, we need to calculate the gradient of the loss with respect to $W$ and $b$\nJust like with forward prop, I will implement two functions. One deals with the back pass for the linear part of the units and the other deals with the derivatives of the activation functions.\nFor the linear part, we take the derivatives of the parameters, obtaining:\n$$ dW^{[l]} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$ $$ db^{[l]} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$ $$ dA^{[l-1]} = W^{[l] T} dZ^{[l]} $$\nFor the activation part, the backprop requires the gradient of the activation function. As such it depends on the activation used, and I\u0026rsquo;ll define them for each one.\nFor sigmoid:\n$$ \\sigma{(z)} = \\frac{1}{1+e^{-x}}$$ $$\\frac{d\\sigma{(z)}}{dz} = \\sigma{(z)}(1-\\sigma{(z)})$$\nFor ReLU:\n$$\\text{ReLU}(z) = \\max{(0,z)}$$ $$\\frac{d\\text{ReLU}}{dz} = \\left\\{\\begin{array}{ll}1 , z \u0026gt; 0\\\\0, z \\le 0\\end{array}\\right.$$\nNote that for ReLU, strictly speaking, there is a discontinuity at $z=0$, however since it is incredibly unlikely that the input to the function will every be exactly zero, it\u0026rsquo;s fine to include it in $z\\le0$\nFor tanh: $$\\tanh{(z)} = \\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$$ $$\\frac{d\\tanh(z)}{dz} = 1-\\tanh^2(z)$$\nFor leaky ReLU: $$\\text{leaky ReLU}(z) = \\max(0.01z, z)$$ $$\\frac{d(\\text{leaky Relu}(z))}{dz} = \\left\\{\\begin{array}{ll}1 , z \u0026gt; 0\\\\0.01, z \\le0\\end{array}\\right.$$\nSo, I\u0026rsquo;ll implement functions for each of these units to compute: $$dZ^{[l]} = dA^{[l]} * g\u0026rsquo;(Z^{[l]})$$\nAdditionally, to initialize backpropagation, we need $\\frac{d\\mathcal{L}}{dA^{[L]}}$, the gradient of the cost function with respect to the last activation output. For cross-entropy this is: $$-\\sum\\limits_{i=1}^{m}\\frac{y^{i}}{a^{[L](i)}} - \\frac{1-y^{i}}{1-a^{[L](i)}}$$\ndef backward_linear(dZ, cache): \u0026quot;\u0026quot;\u0026quot; Arguments: dZ -- Gradient of cost w.r.t linear portion cache -- tuple coming from cached forward prop of layer l Returns: dA_prev -- gradient with respect to activation of previous layer dW -- gradient with respect to weights of current layer db -- gradient with respect to biases of current layer \u0026quot;\u0026quot;\u0026quot; #unpack cache A_prev, W, b = cache #Get number of samples m = A_prev.shape[1] dW = 1./m*np.dot(dZ, A_prev.T) db = 1./m*np.sum(dZ, axis=1, keepdims=True) dA_prev = np.dot(W.T, dZ) assert (dA_prev.shape == A_prev.shape) assert (dW.shape == W.shape) assert (db.shape == b.shape) return dA_prev, dW, db  def backward_activation(dA, Z, activation): \u0026quot;\u0026quot;\u0026quot; Arguments: dA -- post-activation gradient for current layer l Z -- cached matrix from forward prop activation -- the activation to be used in the layer Returns: dZ -- gradient of cost function with respect to Z[l] \u0026quot;\u0026quot;\u0026quot; if activation == 'linear': dZ = dA elif activation == \u0026quot;relu\u0026quot;: dZ = np.array(dA, copy=True) dZ[Z \u0026lt;= 0] = 0 elif activation == \u0026quot;sigmoid\u0026quot;: s = 1./(1+np.exp(-Z)) dZ = dA * s * (1-s) elif activation == \u0026quot;leaky_relu\u0026quot;: dZ = np.array(dA, copy=True) dZ[Z \u0026lt;= 0] = 0.01 elif activation == \u0026quot;tanh\u0026quot;: dZ = dA*(1 - tanh(Z)**2) else: raise NameError(\u0026quot;%s is not a valid activation function\u0026quot; % (activation)) assert(dZ.shape == Z.shape) return dZ  def backward_prop(AL, Y, caches, layer_activations): \u0026quot;\u0026quot;\u0026quot; Implement a backward propagation pass Arguments: AL -- output of the forward propagation Y -- ground truth caches -- list of caches containing linear_cache and activation_cache Returns: grads -- A dictionary with the gradients dA[l], dW[l], db[l] \u0026quot;\u0026quot;\u0026quot; #Define dict to store gradients for parameter update grads = {} L = len(caches) m = AL.shape[1] #Ensure Y is the same as AL (which is essentially y_hat) Y = Y.reshape(AL.shape) #Initialize backprop, a.k.a derivative of cost with respect to AL dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) grads[\u0026quot;dA\u0026quot;+str(L)] = dAL for l in reversed(range(L)): current_cache = caches[l] linear_cache, activation_cache = current_cache dZ = backward_activation(grads[\u0026quot;dA\u0026quot;+str(l+1)],activation_cache, layer_activations[l]) dA_prev, dW, db = backward_linear(dZ, linear_cache) grads[\u0026quot;dA\u0026quot; + str(l)] = dA_prev grads[\u0026quot;dW\u0026quot; + str(l + 1)] = dW grads[\u0026quot;db\u0026quot; + str(l + 1)] = db return grads  Update Parameters The final step is to take the gradients computed in back propagation and use them to update the parameters $W$ and $b$.\nThe method of updating these parameters is important and there are several optimizers that do this in different ways.\n Mini-Batch Gradient Descent: $$ W:=W-\\alpha dW $$ $$ b:=b-\\alpha db $$  For the other optimization algorithms, the concept of exponentially weighted averages becomes an important one. An exponentially weighted average can be calculated with the following formula: $$v_{\\theta, i} := \\beta v_{\\theta, i} + (1-\\beta)\\theta_{i}$$\nWhere $\\theta_{i}$ are the samples in the dataset to average over. The parameter $\\beta$ roughly controls how many samples to average over given by approximately $\\frac{1}{1-\\beta}$. Most commonly in momentum, $\\beta=0.9$, which works out to averaging over the last 10 samples.\n Momentum: $$ \\begin{cases} v_{dW^{[l]}} := \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\nW^{[l]} := W^{[l]} - \\alpha v_{dW^{[l]}} \\end{cases}$$  $$\\begin{cases} v_{db^{[l]}} := \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\nb^{[l]} := b^{[l]} - \\alpha v_{db^{[l]}} \\end{cases}$$\n RMSProp: $$ \\begin{cases} s_{dW^{[l]}} := \\beta s_{dW^{[l]}} + (1 - \\beta) (dW^{[l]})^{2} \\\\\nW^{[l]} := W^{[l]} - \\alpha \\frac{dW^{[l]}}{\\sqrt{s_{dW^{[l]}}}+\\epsilon} \\end{cases}$$  $$\\begin{cases} s_{db^{[l]}} := \\beta s_{db^{[l]}} + (1 - \\beta) (db^{[l]})^{2} \\\\\nb^{[l]} := b^{[l]} - \\alpha \\frac{db^{[l]}}{\\sqrt{s_{db^{[l]}}}+\\epsilon} \\end{cases}$$\nNote the addition of $\\epsilon$ in the denominator in both RMSProp and Adam. That is to prevent NaNs or divisions by zero, it increases numerical stability. The king of the optimizers, Adam, works by combining both momentum and RMSProp. Additionally, it also adds bias correction to the exponentially weighted averages $v$ and $s$. The need for bias correction comes from the fact that as the number of samples that we average over increases, the beginning of the averaging causes the output to be very small since at the start we only have one sample and the others are initialized to zero. As such, the start of our averaging results in a much lower start than the original distribution.\n Adam: $$\\begin{cases} v_{dW^{[l]}} := \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) dW^{[l]} \\\\\nv^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\ns_{dW^{[l]}} := \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (dW^{[l]})^2 \\\\\ns^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\nW^{[l]} := W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon} \\end{cases}$$  $$\\begin{cases} v_{db^{[l]}} := \\beta_1 v_{db^{[l]}} + (1 - \\beta_1) db^{[l]} \\\\\nv^{corrected}_{db^{[l]}} = \\frac{v_{db^{[l]}}}{1 - (\\beta_1)^t} \\\\\ns_{db^{[l]}} := \\beta_2 s_{db^{[l]}} + (1 - \\beta_2) (db^{[l]})^2 \\\\\ns^{corrected}_{db^{[l]}} = \\frac{s_{db^{[l]}}}{1 - (\\beta_2)^t} \\\\\nb^{[l]} := b^{[l]} - \\alpha \\frac{v^{corrected}_{db^{[l]}}}{\\sqrt{s^{corrected}_{db^{[l]}}} + \\varepsilon} \\end{cases}$$\nThe $t$ parameter in Adam included in the bias correction formula is the number of steps taken.\nBesides functions to update these parameters, we also need functions to initialize them (except for gradient descent)\n## Gradient Descent def update_parameters_gd(parameters, grads, learning_rate=0.01): \u0026quot;\u0026quot;\u0026quot; Arguments: parameters -- parameters W and b grads -- gradients from backprop - dW and db Returns: parameters -- parameters W and b updated using gradient descent update rules \u0026quot;\u0026quot;\u0026quot; L = len(parameters) // 2 # number of layers for l in range(L): parameters[\u0026quot;W\u0026quot; + str(l+1)] = parameters[\u0026quot;W\u0026quot; + str(l+1)] - learning_rate*grads[\u0026quot;dW\u0026quot;+str(l+1)] parameters[\u0026quot;b\u0026quot; + str(l+1)] = parameters[\u0026quot;b\u0026quot; + str(l+1)]- learning_rate*grads[\u0026quot;db\u0026quot;+str(l+1)] return parameters ## Momentum def initialize_parameters_momentum(parameters): \u0026quot;\u0026quot;\u0026quot; Arguments: parameters -- dictionary containing parameters W,b Returns: velocities -- initialized velocities for momentum updates \u0026quot;\u0026quot;\u0026quot; L = len(parameters) // 2 velocities = {} # Initialize velocities for l in range(L): velocities[\u0026quot;dW\u0026quot; + str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape) velocities[\u0026quot;db\u0026quot; + str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape) return velocities def update_parameters_momentum(parameters, grads, velocities, learning_rate=0.01, beta=0.9): \u0026quot;\u0026quot;\u0026quot; Arguments: parameters -- parameters W and b grads -- gradients from backprop - dW and db velocities -- current velocities for momentum Returns: parameters -- parameters W and b updated using momentum update rules velocities -- updated velocities \u0026quot;\u0026quot;\u0026quot; L = len(parameters) // 2 for l in range(L): # compute velocities using exponential weighted average velocities[\u0026quot;dW\u0026quot; + str(l+1)] = beta*velocities[\u0026quot;dW\u0026quot;+str(l+1)]+(1-beta)*grads[\u0026quot;dW\u0026quot;+str(l+1)] velocities[\u0026quot;db\u0026quot; + str(l+1)] = beta*velocities[\u0026quot;db\u0026quot;+str(l+1)]+(1-beta)*grads[\u0026quot;db\u0026quot;+str(l+1)] #parameter update parameters[\u0026quot;W\u0026quot; + str(l+1)] = parameters[\u0026quot;W\u0026quot; + str(l+1)] - learning_rate*velocities[\u0026quot;dW\u0026quot; + str(l+1)] parameters[\u0026quot;b\u0026quot; + str(l+1)] = parameters[\u0026quot;b\u0026quot; + str(l+1)] - learning_rate*velocities[\u0026quot;db\u0026quot; + str(l+1)] return parameters, velocities ## RMSProp def initialize_parameters_rmsprop(parameters): \u0026quot;\u0026quot;\u0026quot; Arguments: parameters -- dictionary containing parameters W,b Returns: squares -- initialized moving average of the squared gradient for rmsprop updates \u0026quot;\u0026quot;\u0026quot; L = len(parameters) // 2 squares = {} # Initialize squares for l in range(L): squares[\u0026quot;dW\u0026quot; + str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape) squares[\u0026quot;db\u0026quot; + str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape) return squares def update_parameters_rmsprop(parameters, grads, squares, learning_rate=0.01, beta=0.9, epsilon=1e-8): \u0026quot;\u0026quot;\u0026quot; Arguments: parameters -- parameters W and b grads -- gradients from backprop - dW and db squares -- current squres of past gradients for rmsprop Returns: parameters -- parameters W and b updated using rmsprop update rules squares -- updated squares \u0026quot;\u0026quot;\u0026quot; L = len(parameters) // 2 for l in range(L): # compute velocities using exponential weighted average squares[\u0026quot;dW\u0026quot; + str(l+1)] = beta*squares[\u0026quot;dW\u0026quot;+str(l+1)]+(1-beta)*(grads[\u0026quot;dW\u0026quot;+str(l+1)]**2) squares[\u0026quot;db\u0026quot; + str(l+1)] = beta*squares[\u0026quot;db\u0026quot;+str(l+1)]+(1-beta)*(grads[\u0026quot;db\u0026quot;+str(l+1)]**2) #parameter update parameters[\u0026quot;W\u0026quot; + str(l+1)] = parameters[\u0026quot;W\u0026quot; + str(l+1)] - learning_rate*(grads[\u0026quot;dW\u0026quot;+str(l+1)]/(np.sqrt(squares[\u0026quot;dW\u0026quot; + str(l+1)])+epsilon)) parameters[\u0026quot;b\u0026quot; + str(l+1)] = parameters[\u0026quot;b\u0026quot; + str(l+1)] - learning_rate*(grads[\u0026quot;db\u0026quot;+str(l+1)]/(np.sqrt(squares[\u0026quot;db\u0026quot; + str(l+1)])+epsilon)) return parameters, squares ## Adam def initialize_parameters_adam(parameters): \u0026quot;\u0026quot;\u0026quot; Arguments: parameters -- dictionary containing parameters W,b Returns: velocities -- initialized first gradient weighted averages for adam updates squares -- initialized moving average of the squared gradient for adam updates \u0026quot;\u0026quot;\u0026quot; L = len(parameters) // 2 velocities = {} squares = {} # Initialize velocities and squares for l in range(L): velocities[\u0026quot;dW\u0026quot; + str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape) velocities[\u0026quot;db\u0026quot; + str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape) squares[\u0026quot;dW\u0026quot; + str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape) squares[\u0026quot;db\u0026quot; + str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape) return velocities, squares def update_parameters_adam(parameters, grads, velocities, squares, t, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8): \u0026quot;\u0026quot;\u0026quot; Arguments: parameters -- dictionary with parameters W, b grads -- dictionary with gradients dW, db velocities -- moving average of the first gradient squares -- moving average of the squared gradient t -- counter for bias correction Returns: parameters -- updated parameters according to adam velocities -- updated moving average of the first gradient squares -- updated moving average of the squared gradient \u0026quot;\u0026quot;\u0026quot; L = len(parameters) // 2 v_corrected = {} s_corrected = {} for l in range(L): #Calculate exponentially weighted velocities velocities[\u0026quot;dW\u0026quot; + str(l+1)] = beta1*velocities[\u0026quot;dW\u0026quot; + str(l+1)]+(1-beta1)*grads[\u0026quot;dW\u0026quot; + str(l+1)] velocities[\u0026quot;db\u0026quot; + str(l+1)] = beta1*velocities[\u0026quot;db\u0026quot; + str(l+1)]+(1-beta1)*grads[\u0026quot;db\u0026quot; + str(l+1)] #Bias correction for velocities v_corrected[\u0026quot;dW\u0026quot; + str(l+1)] = velocities[\u0026quot;dW\u0026quot; + str(l+1)]/(1-beta1**t) v_corrected[\u0026quot;db\u0026quot; + str(l+1)] = velocities[\u0026quot;db\u0026quot; + str(l+1)]/(1-beta1**t) #Calculate exponentially weighted squares squares[\u0026quot;dW\u0026quot; + str(l+1)] = beta2*squares[\u0026quot;dW\u0026quot; + str(l+1)]+(1-beta2)*grads[\u0026quot;dW\u0026quot; + str(l+1)]**2 squares[\u0026quot;db\u0026quot; + str(l+1)] = beta2*squares[\u0026quot;db\u0026quot; + str(l+1)]+(1-beta2)*grads[\u0026quot;db\u0026quot; + str(l+1)]**2 #Bias correction for squares s_corrected[\u0026quot;dW\u0026quot; + str(l+1)] = squares[\u0026quot;dW\u0026quot; + str(l+1)]/(1-beta2**t) s_corrected[\u0026quot;db\u0026quot; + str(l+1)] = squares[\u0026quot;db\u0026quot; + str(l+1)]/(1-beta2**t) #Adam parameter updates parameters[\u0026quot;W\u0026quot; + str(l+1)] = parameters[\u0026quot;W\u0026quot; + str(l+1)] - learning_rate*(v_corrected[\u0026quot;dW\u0026quot; + str(l+1)]/(np.sqrt(s_corrected[\u0026quot;dW\u0026quot; + str(l+1)])+epsilon)) parameters[\u0026quot;b\u0026quot; + str(l+1)] = parameters[\u0026quot;b\u0026quot; + str(l+1)] - learning_rate*(v_corrected[\u0026quot;db\u0026quot; + str(l+1)]/(np.sqrt(s_corrected[\u0026quot;db\u0026quot; + str(l+1)])+epsilon)) return parameters, velocities, squares  Combining Everything and Mini-Batch GD After going through each piece, we now need to combine all these functions to train a model. To do this, we have some input data $X$ with respective labels $Y$. Now, to implement mini-bach gradient descent, we need to split $X$ and $Y$ into $m$ mini-batches to run our algorithms on.\ndef mini_batches(X, Y, mini_batch_size = 64, seed = 0): \u0026quot;\u0026quot;\u0026quot; Arguments: X -- input data Y -- corresponding labels mini_batch_size -- size of the mini-batches seed -- used to set np.random.seed differently to get different shuffles Returns: mini_batches -- list of (mini_batch_X, mini_batch_Y) \u0026quot;\u0026quot;\u0026quot; #Set seed np.random.seed(seed) mini_batches = [] #Get number of examples m = X.shape[1] idx = list(np.random.permutation(m)) shuffled_X = X[:, idx] shuffled_Y = Y[idx, :] shuffled_Y = np.reshape(shuffled_Y,(1,m)) assert shuffled_Y.shape == (1,m) #Need to account for when minibatch size is divisible by m num_full_minibatch = int(math.floor(m/mini_batch_size)) for i in range(0, num_full_minibatch): mini_batch_X = shuffled_X[:,mini_batch_size*i: mini_batch_size*(i+1)] mini_batch_Y = shuffled_Y[:,mini_batch_size*i: mini_batch_size*(i+1)] mini_batches.append((mini_batch_X, mini_batch_Y)) # Now need to take care of extra examples of len \u0026lt; m if m % mini_batch_size != 0: mini_batch_X = shuffled_X[:,-(mini_batch_size-m):] mini_batch_Y = shuffled_Y[:,-(mini_batch_size-m):] mini_batches.append((mini_batch_X, mini_batch_Y)) return mini_batches  def train(X, Y, model_shape, layer_activations, optimizer, initialization_method='he', learning_rate = 0.001, mini_batch_size = 64, beta = 0.9, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, num_epochs = 10000, print_cost = True): \u0026quot;\u0026quot;\u0026quot; Implementation of a Neural Network model. Arguments: X -- input data Y -- labels model_shape -- python list with the size of each layer layer_activations -- python list with activation of each layer optimizer -- string corresponding to optimizer to use. One of \u0026quot;gd\u0026quot;,\u0026quot;momentum\u0026quot;,\u0026quot;rmsprop\u0026quot;,\u0026quot;adam\u0026quot; learning_rate -- the learning rate parameter mini_batch_size -- the size of each mini batch beta -- Momentum/RMSProp hyperparameter beta1 -- decay of past gradients parameter for adam beta2 -- decay of past squared gradients for adam epsilon -- hyperparameter preventing division by zero in Adam and RMSProp updates num_epochs -- number of epochs print_cost -- True to print the cost every 5 epochs Returns: parameters -- trained parameters \u0026quot;\u0026quot;\u0026quot; #Track costs costs = [] #Adam bias correction parameter t = 0 #define seed for np.random.seed in mini_batch call seed = np.random.randint(1000) #Number of layers and number of training examples L = len(model_shape) m = X.shape[1] # Initialize parameters parameters = initialize_parameters(model_shape, initialization_method=initialization_method) # Initialize parameters for optimizer if optimizer == \u0026quot;gd\u0026quot;: pass elif optimizer == \u0026quot;momentum\u0026quot;: velocities = initialize_parameters_momentum(parameters) elif optimizer == 'rmsprop': squares = initialize_parameters_rmsprop(parameters) elif optimizer == \u0026quot;adam\u0026quot;: velocities, squares = initialize_parameters_adam(parameters) else: raise NameError(\u0026quot;%s is not a valid optimizer\u0026quot; % (optimizer)) #Loop for i in range(num_epochs): # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch seed = seed + 1 minibatches = mini_batches(X, Y, mini_batch_size, seed) #Get cost over all batchs total_cost = 0 for minibatch in minibatches: # Unpack (minibatch_X, minibatch_Y) = minibatch # Forward propagation pass AL, caches = forward_prop(minibatch_X, layer_activations, parameters) #Get minibatch cost cost_batch = cost(AL, minibatch_Y) #Add to total cost total_cost+=cost_batch # Backward propagation pass grads = backward_prop(AL, minibatch_Y, caches, layer_activations) # Update parameters if optimizer == \u0026quot;gd\u0026quot;: parameters = update_parameters_gd(parameters, grads, learning_rate=learning_rate) elif optimizer == \u0026quot;momentum\u0026quot;: parameters, velocities = update_parameters_momentum(parameters,grads, velocities,learning_rate=learning_rate, beta=beta) elif optimizer == \u0026quot;rmsprop\u0026quot;: parameters, squares = update_parameters_rmsprop(parameters, grads, squares, learning_rate=learning_rate, beta=beta, epsilon=epsilon) elif optimizer == \u0026quot;adam\u0026quot;: #Increment bias correction parameter t = t + 1 parameters, velocities, squares = update_parameters_adam(parameters, grads, velocities, squares, t, learning_rate=learning_rate, beta1=beta1, beta2=beta2, epsilon=epsilon) mean_cost = total_cost / float(mini_batch_size) # Print the cost every 5 epoch if print_cost and i % 5 == 0: print (\u0026quot;Cost after epoch %i: %f\u0026quot; %(i, mean_cost)) if print_cost and i % 1 == 0: costs.append(mean_cost) # plot the cost fig, ax = plt.subplots() fig.set_facecolor('w') fig.set_size_inches(12,9) ax.plot(costs) ax.set_ylabel('Cost') ax.set_xlabel('Epoch') plt.title(\u0026quot;Learning rate = %s, Optimizer = %s\u0026quot; % (learning_rate, optimizer)) plt.show() return parameters  Testing the Model Now that the implementation is complete, let\u0026rsquo;s test the model by doing binary classification on two handwritten digits from the MNIST dataset.\nfrom tensorflow.keras.datasets import mnist (X_train, Y_train), (X_test, Y_test) = mnist.load_data() img_shape = X_train.shape[1:] print('X_train has shape %s\\nY_train has shape %s'%(X_train.shape, Y_train.shape))  X_train has shape (60000, 28, 28) Y_train has shape (60000,)  #Convert Y_train and Y_test to (m,1) Y_train = Y_train.reshape(Y_train.shape[0],1) Y_test = Y_test.reshape(Y_test.shape[0],1) #Visualize one Entry i = np.random.randint(X_train.shape[0]) fig,ax = plt.subplots() fig.set_facecolor('w') ax.imshow(X_train[i]) ax.set_title('Label = ' + str(Y_train[i])) plt.show()  #Choose two classes for our classification model class_a = 3 #Positive Class class_b = 7 #Negative Class #Filter out the dataset to include only images in those classes idx = np.logical_or(np.squeeze(Y_train) == class_a, np.squeeze(Y_train) == class_b) X_train, Y_train = X_train[idx], Y_train[idx] #Assign class_a = 1 and class_b=0 Y_train[np.where(Y_train == class_a)] = 1.00 Y_train[np.where(Y_train == class_b)] = 0.00 print('X_train has shape %s\\nY_train has shape %s'%(X_train.shape, Y_train.shape)) idx = np.logical_or(np.squeeze(Y_test) == class_a, np.squeeze(Y_test) == class_b) X_test, Y_test = X_test[idx], Y_test[idx].astype(np.float64) #Assign class_a = 1 and class_b=0 Y_test[np.where(Y_test == class_a)] = 1.00 Y_test[np.where(Y_test == class_b)] = 0.00 print('X_test has shape %s\\nY_test has shape %s'%(X_test.shape, Y_test.shape))  X_train has shape (12396, 28, 28) Y_train has shape (12396, 1) X_test has shape (2038, 28, 28) Y_test has shape (2038, 1)  #Reshape X_train and X_test into (m, 28*28) X_train_flat = X_train.reshape(X_train.shape[0], -1).T X_test_flat = X_test.reshape(X_test.shape[0], -1).T # Standardize data to have feature values between 0 and 1. X_train_norm = X_train_flat/255. X_test_norm = X_test_flat/255. print (\u0026quot;X_train's shape: \u0026quot; + str(X_train_norm.shape)) print (\u0026quot;X_test's shape: \u0026quot; + str(X_test_norm.shape))  X_train's shape: (784, 12396) X_test's shape: (784, 2038)  Defining our Model I\u0026rsquo;ve chosen to create a model to classify either a $3$ or a $7$. Now, let\u0026rsquo;s define a model.\nThe output is either $1$ or $0$, where $1$ corresponds to a $3$ and $0$ corresponds to a $7$. This means the last layer dimension needs to be $1$. For the first dimension, that should be $28\\times28\\times1=784$, since we\u0026rsquo;re taking the image and stacking each row of pixels ontop of each other (flattening). For our hidden layer, I\u0026rsquo;ll choose $n_h=7$ So we have a three layer model - $784\\times7\\times1$ with layer activations ReLU-ReLU-Sigmoid.\nWe can compare the performance of gradient descent versus adam optimization. Let\u0026rsquo;s start with gradient descent.\n#Model Parameters n_x = X_train_norm.shape[0] n_y = 1 n_h = 7 model_shape = (n_x, n_h, n_y) layer_activations = ['relu','relu','sigmoid'] optimizer = 'gd' learning_rate = 0.0005 parameters = train(X_train_norm,Y_train, model_shape, layer_activations, optimizer, learning_rate=learning_rate, mini_batch_size=128, num_epochs=17)  Cost after epoch 0: 0.458453 Cost after epoch 5: 0.259276 Cost after epoch 10: 0.162094 Cost after epoch 15: 0.085669  Evaluating our Model Now that the model has trained, we need some way of assessing the performance of our model. This is done with our testing set: $(X_{\\text{test}}, Y_{\\text{test}})$ Essentially, we just need to feed $X_{\\text{test}}$ through our model\u0026rsquo;s forward pass, which outputs $A^{[L]}=\\hat{Y}$, our predictions. Then we simply compare $\\hat{Y}$ with $Y_{\\text{test}}$ and evaluate the accuracy as $A=\\frac{\\text{# correct}}{\\text{# total}}$. Additionally, I\u0026rsquo;ll return the indices where the model predicted correctly, and where it predicted incorrectly, to visualize the model\u0026rsquo;s shortcomings.\ndef evaluate(X_test, Y_test, layer_activations, parameters, threshold=0.5): \u0026quot;\u0026quot;\u0026quot; Evaluates performance of trained model on test set Attributes: X_test -- Test set inputs Y_test -- Test set labels layer_activations -- python list of strings corresponding to activation functions of layer l parameters -- trained parameters W, b Returns: correct -- list of booleans corresponding to the indices of correct predictions incorrect -- list of booleans correspondingin to the indices of incorrect predictions \u0026quot;\u0026quot;\u0026quot; #Number of test samples m = X_test.shape[1] assert Y_test.shape == (1,m) Y_pred, _ = forward_prop(X_test, layer_activations, parameters) #Threshold Y_pred[Y_pred\u0026gt;threshold]=1. Y_pred[Y_pred\u0026lt;=threshold]=0 num_correct = np.sum(Y_pred == Y_test) num_incorrect = m-num_correct print(\u0026quot;Accuracy: %f\u0026quot; % (float(num_correct)/m)) correct = Y_pred == Y_test incorrect = Y_pred != Y_test return np.squeeze(correct), np.squeeze(incorrect)  #Evaluate correct, incorrect = evaluate(X_test_norm, Y_test.T, layer_activations, parameters) #Get correect predictions X_correct = X_test[correct] Y_correct = Y_test[correct] #Get incorrect predictions X_incorrect = X_test[incorrect] Y_incorrect = Y_test[incorrect] fig,ax = plt.subplots(3,2) fig.set_size_inches(12,18) fig.set_facecolor('w') i_correct = np.random.randint(len(X_correct), size=3) i_incorrect = np.random.randint(len(X_incorrect), size=3) for i in range(3): ax[i,0].imshow(X_correct[i_correct[i]]) ax[i,0].set_title(\u0026quot;%i: Correctly predicted Y=%i\u0026quot;%(i_correct[i],class_a*Y_correct[i_correct[i]][0] + (1-Y_correct[i_correct[i]][0])*class_b)) ax[i,1].imshow(X_incorrect[i_incorrect[i]]) ax[i,1].set_title(\u0026quot;%i: Incorrectly predicted Y=%i\u0026quot;%(i_incorrect[i],class_b*Y_incorrect[i_incorrect[i]][0] + (1-Y_incorrect[i_incorrect[i]][0])*class_a)) ax[i,0].xaxis.set_visible(False) ax[i,0].yaxis.set_visible(False) ax[i,1].xaxis.set_visible(False) ax[i,1].yaxis.set_visible(False) plt.show()  Accuracy: 0.964671  Adam Optimization Now that we\u0026rsquo;ve gotten results using Gradient Descent, Let\u0026rsquo;s compare it with adam optimization\n#Model Parameters n_x = X_train_norm.shape[0] n_y = 1 n_h = 7 model_shape = (n_x, n_h, n_y) layer_activations = ['relu','relu','sigmoid'] optimizer = 'adam' learning_rate = 0.0005 parameters = train(X_train_norm,Y_train, model_shape, layer_activations, optimizer, learning_rate=learning_rate, mini_batch_size=128, num_epochs=5)  Cost after epoch 0: 0.347253  #Evaluate correct, incorrect = evaluate(X_test_norm, Y_test.T, layer_activations, parameters) #Get correect predictions X_correct = X_test[correct] Y_correct = Y_test[correct] #Get incorrect predictions X_incorrect = X_test[incorrect] Y_incorrect = Y_test[incorrect] fig,ax = plt.subplots(3,2) fig.set_size_inches(12,18) fig.set_facecolor('w') i_correct = np.random.randint(len(X_correct), size=3) i_incorrect = np.random.randint(len(X_incorrect), size=3) for i in range(3): ax[i,0].imshow(X_correct[i_correct[i]]) ax[i,0].set_title(\u0026quot;%i: Correctly predicted Y=%i\u0026quot;%(i_correct[i],class_a*Y_correct[i_correct[i]][0] + (1-Y_correct[i_correct[i]][0])*class_b)) ax[i,1].imshow(X_incorrect[i_incorrect[i]]) ax[i,1].set_title(\u0026quot;%i: Incorrectly predicted Y=%i\u0026quot;%(i_incorrect[i],class_b*Y_incorrect[i_incorrect[i]][0] + (1-Y_incorrect[i_incorrect[i]][0])*class_a)) ax[i,0].xaxis.set_visible(False) ax[i,0].yaxis.set_visible(False) ax[i,1].xaxis.set_visible(False) ax[i,1].yaxis.set_visible(False) plt.show()  Accuracy: 0.969578  Comparison As we can see, using the adam optimizer yielded better accuracy in nearly one third of the number of epochs.\n","date":1591765774,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591765774,"objectID":"cb30a4de54be5496c51334908322df12","permalink":"https://Fquico1999.github.io/project/numpy_neural_nets/","publishdate":"2020-06-09T22:09:34-07:00","relpermalink":"/project/numpy_neural_nets/","section":"project","summary":"Implementation of Neural Networks using only NumPy","tags":["Deep Learning"],"title":"NumPy Neural Networks","type":"project"},{"authors":[],"categories":[],"content":" Click here if you want to skip to my involvement in this team.\nOverview  UBC Open Robotics is a student team comprised of 60 students split into three subteams - ArtBot, PianoBot, and Robocup@Home. I am a member of the software team in the RoboCup@Home subteam.\nThe objective of RoboCup@Home is to build a household assistant robot that can perform a variety of tasks, including carrying bags, introducing and seating guests at a party, answering a variety of trivia questions and more. Open Robotics is developing a robot to compete in the 2021 RoboCup@Home Education Challenge while in the meantime, our subteam will compete in the 2020 Competition using the Turtlebot 2 as our hardware platform.\nThe Challenge  The rules for the 2020 Challenge can be found here, but they boil down to three specific tasks:\n Carry My Luggage - Navigation task Find My Mates - Vision task Receptionist - Speech task  Carry My Luggage Goal: The robot helps the operator to carry a bag to the car parked outside\nStarting at a predifined location, the robot has to find the operator and pick up the bag the operator is pointing to. After picking up the bag, the robot needs to indicate that it is ready to follow and then it must follow the operator while facing 4 obstacles along the way (crowd, small object, difficult to see 3D object, small blocked area).\nFind My Mates Goal: The robot fetches the information of the party guests for the operator who knows only the names of the guests.  Knowing only the operator, the robot must identify unknown people and meet those that are waving. Afterwards, it must remember the person and provide a unique description of that person, as well as that person\u0026rsquo;s location, to the operator.\nReceptionist Goal: The robot has to take two arriving guests to the living room, introducing them to each other, and offering the just-arrived guest an unoccupied place to sit.  Knowing the host of the party, John, the robot must identify unknown guests, request their names and favourite drinks and then point to an empty seat where the guest can sit.\nMy Contributions My main contributions have been in speech recognition and in handle segmentation, targeting task 3 and task 1 respectively, however I also worked on facial recognition earlier in the project.\nSpeech Recognition You can find this repository here\nSpeech recognition is implemented using PocketSphinx which is based on CMUSphinx. Which offers two modes of operation - Keyword Spotting (KWS) and Language Model (LM).\nKWS Keyword spotting tries to detect specific keywords or phrases, without imposing any type of grammer rules ontop. Utilizing keyword spotting requires a .dic file and a .kwslist file.\nThe dictionary file is a basic text file that contains all the keywords and their phonetic pronunciation, for instance:\nBACK\tB AE K FORWARD\tF AO R W ER D FULL\tF UH L  These files can be generated here .\nThe .kwslist file has each keyword and a certain threshold, more or less corresponding to the length of the word or phrase, as follows:\nBACK /1e-9/ FORWARD /1e-25/ FULL SPEED /1e-20/  LM Language model mode additionally imposes a grammer. To utilize this mode, .dic, .lm and .gram files are needed.\nThe dictionary file is the same as in KWS mode.\nThe .lm file can be generated, along with the .dic file, from a corpus of text, using this tool\nThe generate_corpus.py script in SpeechRecognition/asr/resources sifts through the resource files from robocup\u0026rsquo;s GPSRCmdGenerator and creates a corpus. The .dic and .lm files are generated from it by using the above tool.\nFinally, the .gram file specifies the grammer to be imposed. For instance, if the commands we are expecting are always an action followed by an object or person and then a location, it might look like:\npublic \u0026lt;rule\u0026gt; = \u0026lt;actions\u0026gt; [\u0026lt;objects\u0026gt;] [\u0026lt;names\u0026gt;] [\u0026lt;locations\u0026gt;]; \u0026lt;actions\u0026gt; = MOVE | STOP | GET | GIVE \u0026lt;objects\u0026gt; = BOWL | GLASS \u0026lt;names\u0026gt; = JOE | JOEBOB \u0026lt;locations\u0026gt; = KITCHEN | BEDROOM  Handle Segmentation You can find this repository here\n ","date":1591395502,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591395502,"objectID":"ff62214a94179c9a208c44ab9465861e","permalink":"https://Fquico1999.github.io/project/openrobotics/","publishdate":"2020-06-05T15:18:22-07:00","relpermalink":"/project/openrobotics/","section":"project","summary":"Developing Software to compete in the RoboCup@Home competition.","tags":["Deep Learning","Computer Vision"],"title":"UBC Open Robotics","type":"project"},{"authors":[],"categories":[],"content":"Overview ENPH 353 is a project course designed to teach machine learning techinques with an end-of-term competition. The premise of the competition is to develop algorithms that allow a simulated robot to traverse a parking lot and correctly identify locations and number plates of parked cars while avoiding pedestrians and a moving vehicle. The simulation took place in Gazebo in ROS\nThe Competition The image above shows the parking lot for the competition. The robot is the white, square car. It\u0026rsquo;s task is to drive on the roads while collecting the license plates on the blue rectangular cars. Additionally, it must avoid pedestrians and the truck driving around the inside track.\n Example license plate  The license plates hold two pieces of information, the position of the car marked with the larger P1 above, and a BC auto-generated license plate with two letters and two numbers.\nThe inputs to the system were the images captured by a camera mounted on the robot\u0026rsquo;s frame and as outputs the robot would publish velociy commands to guide the robot as well as positions and license plate data to a server for scoring.\nThe scores are determined by the following:\n   Rules Points     Correctly record license plate and position for a car on outside track +6   Correctly record license plate and position for a car on inside track +8   More than two wheels outside of the track -2   Collide with white pick-up truck -5   Hit pedestrian -10   Drive one full loop around the outer track +5    The Strategy YOLO I decided to use the YOLO framework to allow the robot to understand it\u0026rsquo;s environment. Yolo stands for \u0026ldquo;You Only Look Once\u0026rdquo;, and is a state of the art object detection system. I used YOLOv3 to obtain labeled bounding boxes around classes of interest, namely the blue parked cars, pedestrians, the truck, and license plates.\nYOLO works by taking an image and dividing into smaller subsections, and predicting locations and accuracies for bounding boxes of a certain class. The advantage of using YOLO is that it is incredibly fast compared to other classifier models, allowing us to obtain near real-time predictions.\nTraining the model required around 200 labeled images taken from simulation video, trained for about 25000 iterations. In ROS, a node subscribes to the camera feed and passes the images through yolo. A YoloElement message was made to store each bounding box for each class, and publish it to a yolo node. This node informs pedestrian logic and gives bounding boxes for the license plate detection as well.\n YOLO Output - The robot is waiting at the cross section. It detects the pedestrian as well as the car and license plate ahead.  Navigation The main components of the robot\u0026rsquo;s navigation are the driver and controller.\nDriver The essential method for Karen’s driving was get_twist(). This method used computer vision techniques to return a Twist command (Twist is a message that contains all the velocities of the robot) which would be called by the controller to drive the robot. The driver has three main approaches to following the road.\nThe first two approaches are very similar. The robot can follow the road by either looking at the right edge or the left edge of the road and following it. These approaches are mirror, so the following is a list of steps taken to perform right edge following:\n Scale input image to a smaller size and apply HSV mask to filter out the road. Find the first pixel of a road from the right-hand side at an upper and lower location. Compare these pixel locations to the expected locations to determine the current error. If the error magnitude exceeds a threshold, turn left if the error is negative, or right if the error is positive, otherwise, drive straight.   Driving Straight - the relative difference in white lines is within the threshold. Left Turn - the relative difference causes a negative error, robot will turn left.  This method was found to be robust. Even when starting off the road, the robot will turn and navigate towards the road, and begin following the edge. However, general navigation and knowing which way to drive is not solved by this approach. The controller must solve these challenges. Note, to follow the left edge, the white lines are flipped about the y-axis in the above figures.\nThe third approach of road following is to use the “center of mass” of the road. This method is not as robust as the above edge following. However, this approach is necessary when the edges of the road are misleading. This approach follows a similar idea as edge following, except it differs in steps 2 and 3:\nThreshold the image so that the road is a binary mask. Use OpenCV to compute the moments of the image, then compare the x coordinate of the center of mass of the road with the center of the image to get the error.  In general, each of these approaches could follow the road successfully. It is up to the controller to decide when to use each approach.\nController The robot\u0026rsquo;s controller makes decisions about when and where to turn, when to stop for pedestrians, and when to stop for the pick-up truck. The following is a flow chart illustrating the state diagram of the controller:\nExit T-IntersectionExit T-IntersectionRight Edge Follow PerimeterRight Edge Follow\u0026hellip;NoNoSee Pedestrian?See Pedestrian?Wait Untill CrossedWait Untill CrossedYesYesCollected Last Perimiter License Plate?Collected Last\u0026hellip;NoNoYesYesLeft Edge Follow\n(Enter Inner Track)Left Edge Follow\u0026hellip;In Inner\nRing?In Inner\u0026hellip;NoNoFollow Road CMFollow Road CMYesYesCollected Last Perimiter License Plate?Collected Last\u0026hellip;FinishedFinishedNoNoYesYesTruck Close?Truck Close?Wait Untill GoneWait Untill GoneYesYesInitialize YOLO\nand License Plate ReaderInitialize YOLO\u0026hellip;Viewer does not support full SVG 1.1\nPosition and License Plate Recognition License Plates The algorithm takes cropped license plate images based on bounding box predictions from YOLO and does some preprocessing before passing them into a CNN for character recognition.\nThe preprocessing algorithm takes bounding boxes from /yolo with the license plate class and crops the raw camera image to size. We obtain potential characters using and adaptive threshold followed by cv2\u0026rsquo;s findContours() function. After some filtering based on size and aspect ratio, we end up with four predictions. The ordering of characters is determined based on the x position of the bounding box prediction.\n License Plate Recognition - After adaptive thresholding, findContours yields potential character candidates that are filtered producing the final 4 characters seen.  Position To read the positions of each license plate, a region of interest is defined based on the bounding box around the license plate from YOLO. To perform character recognition, the CNN is used again, trained on data collected from allowing the robot to do several laps around the perimeter.\n Examples of positions after cropping to ROI  Results A total of 20 teams competed in this competition. This model was one of four to receive a perfect score of 57 points.\n The video above shows the robot completing the outer ring. The Gazebo simulation is shown on the right, the scoring server is on the bottom left, and the terminal displaying information about the robot is on the upper left.\n","date":1591395484,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591395484,"objectID":"c5502dda16902d094db7bd7a7a1a664d","permalink":"https://Fquico1999.github.io/project/enph353/","publishdate":"2020-06-05T15:18:04-07:00","relpermalink":"/project/enph353/","section":"project","summary":"Implemeted YOLO to navigate a simulated course for ENPH 353.","tags":["Deep Learning","Computer Vision"],"title":"Machine Learning Competition","type":"project"},{"authors":[],"categories":[],"content":"See the paper here\n","date":1591395470,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591395470,"objectID":"76a23cb2e44a62212c48b3b816da0d43","permalink":"https://Fquico1999.github.io/project/eece571t/","publishdate":"2020-06-05T15:17:50-07:00","relpermalink":"/project/eece571t/","section":"project","summary":"A Project for EECE 571T - Advanced Machine Learning Tools - Where I created a pipeline to detect FOXP3+ biomarkers in follicular lymphoma TMA cores.","tags":["Deep Learning"],"title":"Artifact Removal \u0026 Biomarker Segmentation","type":"project"}]