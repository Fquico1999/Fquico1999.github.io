<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jiwon Kim | Francisco Farinha</title>
    <link>https://Fquico1999.github.io/author/jiwon-kim/</link>
      <atom:link href="https://Fquico1999.github.io/author/jiwon-kim/index.xml" rel="self" type="application/rss+xml" />
    <description>Jiwon Kim</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â©2020 Francisco Farinha | [Academic](https://sourcethemes.com/academic/) | [Hugo](https://gohugo.io)</copyright><lastBuildDate>Fri, 26 Jun 2020 10:37:12 -0700</lastBuildDate>
    <image>
      <url>https://Fquico1999.github.io/images/icon_hu59cf491a3a20dd4a23becc20dcebe21e_42632_512x512_fill_lanczos_center_2.png</url>
      <title>Jiwon Kim</title>
      <link>https://Fquico1999.github.io/author/jiwon-kim/</link>
    </image>
    
    <item>
      <title>Accurate Image Super-Resolution Using Very Deep Convolutional Networks</title>
      <link>https://Fquico1999.github.io/post/vdsr_paper/</link>
      <pubDate>Fri, 26 Jun 2020 10:37:12 -0700</pubDate>
      <guid>https://Fquico1999.github.io/post/vdsr_paper/</guid>
      <description>&lt;p&gt;Find the paper 
&lt;a href=&#34;https://arxiv.org/abs/1511.04587&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;at-a-glance&#34;&gt;At a Glance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The paper proposes an effective model for single image super resolution that is highly accurate.&lt;/li&gt;
&lt;li&gt;Increasing the model depth increases overall accuracy.&lt;/li&gt;
&lt;li&gt;Contextual information over large regions is built up by cascading multiple smaller filters.&lt;/li&gt;
&lt;li&gt;Convergence speed is maximized by learning only residuals, and using large learning rates with adjustable gradient clipping.&lt;/li&gt;
&lt;li&gt;May be usefull in denoising and compression artifact removal&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The goal of the paper is to introduce a single image super resolution (SISR) model that addresses some of the limitations of a previously proposed framework, the SRCNN.&lt;/p&gt;
&lt;p&gt;The advantages of using CNNs for super resolution is that they provide an effective end-to-end solution, whereas past work required hand-engineered features.&lt;/p&gt;
&lt;p&gt;The paper lists three limitations of SRCNNs and how VDSR can address these:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;SRCNN is context dependent in small images&lt;/em&gt; - Information in a small patch does not hold enough information for detail recovery. VDSR addresses this by cascading small filters to capture large region information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Training for deep CNNs is slow&lt;/em&gt; - VDSR addresses this by only learning &lt;em&gt;residuals&lt;/em&gt; - the difference between the Low Resolution (LR) and High Resolution (HR) images. This works because the LR and HR images share the same information to a very large extent. Additionally, very large learning rates are used during training, with adjustable gradient clipping.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;SRCNN only works for a single scale&lt;/em&gt; - A single VDSR model is adequate for multi-scale-factor super resolution.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;proposed-method&#34;&gt;Proposed Method&lt;/h2&gt;
&lt;h3 id=&#34;proposed-network&#34;&gt;Proposed Network&lt;/h3&gt;
&lt;p&gt;The network takes in an interpolated LR (ILR) image of shape $w \times h \times 3$ and predicts the residual image ($ w \times h \times 1$) which is then added onto the ILR to yield the HR image ($w \times h \times 3$).&lt;/p&gt;
&lt;p&gt;The network is comprised of $L$ layers where all but $l=1,20$ (first and last) follow ZEROPAD -&amp;gt; CONV($3\times 3, 64 \text{ filters}$) -&amp;gt; RELU. The first layer operates on the input and the last layer consists of ZEROPAD -&amp;gt; CONV($3\times 3, 1 \text{ filter}$) to output the desired residual image.&lt;/p&gt;
&lt;p&gt;The purpose of zero-padding before each convolution is to preserve the size of the feature maps. One issue with deep CNNs is that the convolution operation reduces the size of the feature map. Pixels on the border cannot be inferred properly, so usually SISR methods crop the boundary out which is fine for shallow models, but for deep CNNs it is unfeasible. Zero-padding addresses this issue, and is reported to work well.&lt;/p&gt;
&lt;p&gt;$L$ is specified to be $20$ in the paper&amp;rsquo;s training description.&lt;/p&gt;
&lt;h3 id=&#34;training&#34;&gt;Training&lt;/h3&gt;
&lt;p&gt;The Loss function was the mean squared error averaged over the training set: $\frac{1}{2}  || \pmb{y} - \pmb{\hat{y}}||^2$, where $\pmb{y}$ is the HR image corresponding to the input LR image, and $\pmb{\hat{y}}$ is the model predicted HR image.&lt;/p&gt;
&lt;h4 id=&#34;residual-learning&#34;&gt;Residual Learning&lt;/h4&gt;
&lt;p&gt;The residual image is defined as $\pmb{r}=\pmb{y}-\pmb{x}$. Most values are likely to be small or zero, which is desirable when training. Since we want the network to predict the residual $\pmb{r}$, the loss function can be rewritten as $\frac{1}{2}  || \pmb{r} - \pmb{\hat{y}}||^2$. However, in the actual network training, the loss is the $L_2$ norm betweeen the reconstructed image $\pmb{r}+\pmb{x}$ and the ground truth $\pmb{y}$.&lt;/p&gt;
&lt;p&gt;Mini-batch Gradient Descent was used with a momentum optimizer (I assume, as the paper references momentum $\beta = 0.9$, could also be the Adam optimizer) and a weight decay of $0.0001$ (weight decay means adding a regularizing term to the loss, $\mathcal{L} = \frac{1}{2}  || \pmb{y} - \pmb{\hat{y}}||^2 + \gamma L_2, \gamma=0.0001$)&lt;/p&gt;
&lt;h4 id=&#34;adjustable-gradient-clipping&#34;&gt;Adjustable Gradient Clipping&lt;/h4&gt;
&lt;p&gt;An issue when training deep CNNs is the slow speed of convergence. One tactic to speed up training is to increase the learning rate $\alpha$, however this can lead to exploding gradients.&lt;/p&gt;
&lt;p&gt;One solution to this is referred to as Gradient Clipping where the gradients of the parameters with respect to the loss function are clipped between a certain range $[-\theta, \theta]$. The issue with this approach is that, at the start of training when the learning rate is very high, $\theta$ must be very small to prevent exploding gradients, however as the network is trained, learning rate is annealed and as such $\alpha \frac{\partial{\mathcal{L}}}{\partial{W}}$ gets increasingly smaller.&lt;/p&gt;
&lt;p&gt;The suggested method is to set gradients between $[-\frac{\theta}{\alpha}, \frac{\theta}{\alpha}]$, so the clipping is adjusted based on the current learning rate.&lt;/p&gt;
&lt;h4 id=&#34;multi-scale&#34;&gt;Multi-Scale&lt;/h4&gt;
&lt;p&gt;The model can be adapted to handle mutliple scales by simply training it on data of varying scales.
Images are divided into sub-images without overlap where sub-images from different scales are present.&lt;/p&gt;
&lt;p&gt;The paper tests the performance of a model trained with $s_{train}=\{2\}$ (scale factor of 2 in the training set) on different input scales and sees that for $s_{train} \ne s_{test}$, performance is bad. However when $s_{train}=\{2,3,4\}$ the performance at each scale factor is comparable with a corresponding single-scale network, even outperforming single-scale models at large scales (3,4).&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;VDSR outperforms Bicubic, A+, RFL, SelfEx, and SRCNN (all methods listed) in every regard (PSNR/SSIM/time).&lt;/p&gt;
&lt;p&gt;Benchmarks were made on Set5, Set14, B100 and Urban100 datasets.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
